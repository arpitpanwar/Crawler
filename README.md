# Crawler
The project contains an XPath parser which parses a particular grammar and a Web crawler.
Web crawler can be used to parse any sites and is complaint with most robots.txt rules. It stores the fetched pages in Berkeley DB.
The project also contains a web interface where a user can create an account and provide multiple xpath expressions and a url where the crawler should crawl in order to fetch pages which have content which matches the provided xpath expression.
The matching pages are then displayed to the user using a provided xslt.


